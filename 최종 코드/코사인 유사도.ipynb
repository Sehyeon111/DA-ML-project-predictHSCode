{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f30bd2-75f6-4a0c-b794-70b4f3dc1460",
   "metadata": {},
   "source": [
    "# 사전 데이터 불러오기 + 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39db3dcd-7bfc-4dce-baac-22889c054dd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '프로젝트/사전/all_keywords.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m프로젝트/사전/all_keywords.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '프로젝트/사전/all_keywords.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('프로젝트/사전/all_keywords.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a279266-aaa0-4b25-bee2-22a3a43e4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3c599-104b-4eef-8364-f7e4235d22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd29ce-1e96-4b35-8d9c-ab8be46d9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('hs_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61585db9-5c16-4cb7-8a8d-a5eb8f66db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = df.to_dict()\n",
    "flattened_dict = {key: inner_value for inner_dict in result_dict.values() for key, inner_value in inner_dict.items()}\n",
    "\n",
    "# print(flattened_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa0435-4e01-4569-a7de-cbbb09de7fc6",
   "metadata": {},
   "source": [
    "# 테스트 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8997c8-c52c-4118-b979-ffbce7721400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('./프로젝트/사전/dict_test_total.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c231498-64f6-4df3-978d-e7535c07f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hs code'] = data['hs code'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57280ac5-002c-49d0-b731-2e2d57029eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669431bf-13a1-489d-93c9-3ad830ef412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# hs 코드 체크 함수\n",
    "def check_code(code):\n",
    "    p = re.compile('8[45]\\w*')  # 84로 시작하는 패턴\n",
    "    m = p.match(code)  # hs코드에서 패턴에 맞는 거 찾기 \n",
    "    \n",
    "    if m is None:  # 패턴에 맞지 않는 경우(패턴에 맞지 않으면 none 출력)\n",
    "        return None  # 패턴에 맞지 않으면 na값으로 반환\n",
    "    \n",
    "    else:  # 패턴에 맞는 경우\n",
    "        return m.group()  # 패턴에 맞는 코드(4자리수) 반환\n",
    "\n",
    "# hs code 열에 hs 코드 체크 함수 적용\n",
    "data['hs code'] = data['hs code'].apply(lambda x : check_code(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7f3c0-60da-46b4-8c2d-9b0f26c0808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcfabc-1739-4d2c-b9d0-c7b856cc1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ed4bb-a045-4f41-8b81-c80bb0a2fab4",
   "metadata": {},
   "source": [
    "# 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e8594-3f10-424a-83da-66a22ae5f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # HS 코드 문서와 입력 설명서를 하나로 합침\n",
    "    documents = list(hs_code_documents.values())\n",
    "    documents.append(input_document)\n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    \n",
    "    # description\n",
    "    tfidf_matrix = tfidf.fit_transform(documents)\n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드 인덱스\n",
    "    most_similar_hs_code_index = similarities.argmax()\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드와 유사도 값\n",
    "    most_similar_hs_code = list(hs_code_documents.keys())[most_similar_hs_code_index]\n",
    "    similarity_value = similarities[0][most_similar_hs_code_index]\n",
    "\n",
    "    \n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"유사: {most_similar_hs_code}\")\n",
    "    # print(f\"유사도: {similarity_value}\")\n",
    "    if int(num) == most_similar_hs_code:\n",
    "        cnt += 1 \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d289acd-6691-4a9b-983b-b6fcbcaf9b31",
   "metadata": {},
   "source": [
    "# 전처리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1c558-488e-45fb-8473-63699eac61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "# # 필요한 nltk 데이터 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # 구두점 제거 함수\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 불용어 제거 및 단어의 형태소 분석을 위한 객체 생성\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 입력 설명서 전처리\n",
    "    input_document = remove_punctuation(input_document)\n",
    "    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화\n",
    "    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]\n",
    "    \n",
    "    # HS 코드 문서 전처리\n",
    "    processed_hs_code_documents = {}\n",
    "    for code, doc in hs_code_documents.items():\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokens = word_tokenize(doc.lower())  # 소문자로 변환 및 토큰화\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        processed_hs_code_documents[code] = ' '.join(tokens)\n",
    "    \n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    documents = list(processed_hs_code_documents.values())\n",
    "    documents.append(' '.join(input_tokens))  # 입력 설명서를 단일 문자열로 추가\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix = tfidf.fit_transform(documents)\n",
    "    \n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드 인덱스\n",
    "    most_similar_hs_code_index = similarities.argmax()\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드와 유사도 값\n",
    "    most_similar_hs_code = list(hs_code_documents.keys())[most_similar_hs_code_index]\n",
    "    similarity_value = similarities[0][most_similar_hs_code_index]\n",
    "    \n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"유사: {most_similar_hs_code}\")\n",
    "    # print(f\"유사도: {similarity_value}\")\n",
    "    if int(num) == most_similar_hs_code:\n",
    "        cnt += 1 \n",
    "print('맞춘개수: ',cnt)\n",
    "print('예측 정확도: ', (cnt/len(data['name_des']))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8ca7a-90c9-43f6-98f3-d390fae8e77c",
   "metadata": {},
   "source": [
    "# 가중치 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecab7d9-d20e-45f6-b753-7b77d0d5c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "# # 필요한 nltk 데이터 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # 구두점 제거 함수\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 불용어 제거 및 단어의 형태소 분석을 위한 객체 생성\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 입력 설명서 전처리\n",
    "    input_document = remove_punctuation(input_document)\n",
    "    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화\n",
    "    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]\n",
    "    \n",
    "    # HS 코드 문서 전처리\n",
    "    processed_hs_code_documents = {}\n",
    "    for code, doc in hs_code_documents.items():\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokens = word_tokenize(doc.lower())  # 소문자로 변환 및 토큰화\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        processed_hs_code_documents[code] = ' '.join(tokens)\n",
    "    \n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    documents = list(processed_hs_code_documents.values())\n",
    "    tfidf_matrix = tfidf.fit(documents)\n",
    "    documents.append(' '.join(input_tokens))  # 입력 설명서를 단일 문자열로 추가\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix = tfidf.transform(documents)\n",
    "    # tfidf_matrix[:, tfidf.vocabulary_['solely']] *= 2.0\n",
    "    # tfidf_matrix[:, tfidf.vocabulary_['principally']] *= 2.0\n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드 인덱스\n",
    "    most_similar_hs_code_index = similarities.argmax()\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드와 유사도 값\n",
    "    most_similar_hs_code = list(hs_code_documents.keys())[most_similar_hs_code_index]\n",
    "    similarity_value = similarities[0][most_similar_hs_code_index]\n",
    "    \n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"유사: {most_similar_hs_code}\")\n",
    "    # print(f\"유사도: {similarity_value}\")\n",
    "    if int(num) == most_similar_hs_code:\n",
    "        cnt += 1 \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bec69f-a5fd-423f-b765-93e4166e437c",
   "metadata": {},
   "source": [
    "# 상위 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c81ed-826e-4009-8c0a-bcd16fe7ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# # 필요한 nltk 데이터 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # 구두점 제거 함수\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 불용어 제거 및 단어의 형태소 분석을 위한 객체 생성\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 입력 설명서 전처리\n",
    "    input_document = remove_punctuation(input_document)\n",
    "    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화\n",
    "    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]\n",
    "    \n",
    "    # HS 코드 문서 전처리\n",
    "    processed_hs_code_documents = {}\n",
    "    for code, doc in hs_code_documents.items():\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokens = word_tokenize(doc.lower())  # 소문자로 변환 및 토큰화\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        processed_hs_code_documents[code] = ' '.join(tokens)\n",
    "    \n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    documents = list(processed_hs_code_documents.values())\n",
    "    tfidf_matrix = tfidf.fit(documents)\n",
    "    documents.append(' '.join(input_tokens))  # 입력 설명서를 단일 문자열로 추가\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix = tfidf.transform(documents)\n",
    "\n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "\n",
    "    \n",
    "    # 유사도가 가장 높은 상위 3개 HS 코드 인덱스 및 값\n",
    "    most_similar_hs_code_indices = np.argsort(similarities)[0][-3:][::-1]  # 상위 3개 인덱스\n",
    "    top_3_similarity_values = similarities[0][most_similar_hs_code_indices] # 상위 3개 유사도 값\n",
    "    top_3_hs_codes = [list(hs_code_documents.keys())[i] for i in most_similar_hs_code_indices]  # 상위 3개 HS 코드\n",
    "    \n",
    "    \n",
    "    # 원래 HS 코드 출력\n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"상위 3개 유사도 값: {top_3_similarity_values}\")\n",
    "    # print(f\"상위 3개 HS 코드: {top_3_hs_codes}\")\n",
    "    \n",
    "    # 입력 HS 코드와 유사한 상위 3개 HS 코드 중 일치하는 경우 cnt 증가\n",
    "    if int(num) in top_3_hs_codes:\n",
    "        cnt += 1\n",
    "\n",
    "print('맞춘개수: ', cnt)\n",
    "print('예측 정확도: ', (cnt/len(data['name_des']))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
