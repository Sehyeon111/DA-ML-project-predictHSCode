{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f30bd2-75f6-4a0c-b794-70b4f3dc1460",
   "metadata": {},
   "source": [
    "# 사전 데이터 불러오기 + 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db3dcd-7bfc-4dce-baac-22889c054dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('프로젝트/사전/all_keywords.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a279266-aaa0-4b25-bee2-22a3a43e4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3c599-104b-4eef-8364-f7e4235d22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd29ce-1e96-4b35-8d9c-ab8be46d9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('hs_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61585db9-5c16-4cb7-8a8d-a5eb8f66db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = df.to_dict()\n",
    "flattened_dict = {key: inner_value for inner_dict in result_dict.values() for key, inner_value in inner_dict.items()}\n",
    "\n",
    "# print(flattened_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa0435-4e01-4569-a7de-cbbb09de7fc6",
   "metadata": {},
   "source": [
    "# 테스트 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8997c8-c52c-4118-b979-ffbce7721400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('./프로젝트/사전/dict_test_total.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c231498-64f6-4df3-978d-e7535c07f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hs code'] = data['hs code'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57280ac5-002c-49d0-b731-2e2d57029eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669431bf-13a1-489d-93c9-3ad830ef412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# hs 코드 체크 함수\n",
    "def check_code(code):\n",
    "    p = re.compile('8[45]\\w*')  # 84로 시작하는 패턴\n",
    "    m = p.match(code)  # hs코드에서 패턴에 맞는 거 찾기 \n",
    "    \n",
    "    if m is None:  # 패턴에 맞지 않는 경우(패턴에 맞지 않으면 none 출력)\n",
    "        return None  # 패턴에 맞지 않으면 na값으로 반환\n",
    "    \n",
    "    else:  # 패턴에 맞는 경우\n",
    "        return m.group()  # 패턴에 맞는 코드(4자리수) 반환\n",
    "\n",
    "# hs code 열에 hs 코드 체크 함수 적용\n",
    "data['hs code'] = data['hs code'].apply(lambda x : check_code(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7f3c0-60da-46b4-8c2d-9b0f26c0808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcfabc-1739-4d2c-b9d0-c7b856cc1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ed4bb-a045-4f41-8b81-c80bb0a2fab4",
   "metadata": {},
   "source": [
    "# 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e8594-3f10-424a-83da-66a22ae5f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # HS 코드 문서와 입력 설명서를 하나로 합침\n",
    "    documents = list(hs_code_documents.values())\n",
    "    documents.append(input_document)\n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    \n",
    "    # description\n",
    "    tfidf_matrix = tfidf.fit_transform(documents)\n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드 인덱스\n",
    "    most_similar_hs_code_index = similarities.argmax()\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드와 유사도 값\n",
    "    most_similar_hs_code = list(hs_code_documents.keys())[most_similar_hs_code_index]\n",
    "    similarity_value = similarities[0][most_similar_hs_code_index]\n",
    "\n",
    "    \n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"유사: {most_similar_hs_code}\")\n",
    "    # print(f\"유사도: {similarity_value}\")\n",
    "    if int(num) == most_similar_hs_code:\n",
    "        cnt += 1 \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d289acd-6691-4a9b-983b-b6fcbcaf9b31",
   "metadata": {},
   "source": [
    "# 전처리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1c558-488e-45fb-8473-63699eac61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "# # 필요한 nltk 데이터 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # 구두점 제거 함수\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 불용어 제거 및 단어의 형태소 분석을 위한 객체 생성\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 입력 설명서 전처리\n",
    "    input_document = remove_punctuation(input_document)\n",
    "    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화\n",
    "    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]\n",
    "    \n",
    "    # HS 코드 문서 전처리\n",
    "    processed_hs_code_documents = {}\n",
    "    for code, doc in hs_code_documents.items():\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokens = word_tokenize(doc.lower())  # 소문자로 변환 및 토큰화\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        processed_hs_code_documents[code] = ' '.join(tokens)\n",
    "    \n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    documents = list(processed_hs_code_documents.values())\n",
    "    documents.append(' '.join(input_tokens))  # 입력 설명서를 단일 문자열로 추가\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix = tfidf.fit_transform(documents)\n",
    "    \n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드 인덱스\n",
    "    most_similar_hs_code_index = similarities.argmax()\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드와 유사도 값\n",
    "    most_similar_hs_code = list(hs_code_documents.keys())[most_similar_hs_code_index]\n",
    "    similarity_value = similarities[0][most_similar_hs_code_index]\n",
    "    \n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"유사: {most_similar_hs_code}\")\n",
    "    # print(f\"유사도: {similarity_value}\")\n",
    "    if int(num) == most_similar_hs_code:\n",
    "        cnt += 1 \n",
    "print('맞춘개수: ',cnt)\n",
    "print('예측 정확도: ', (cnt/len(data['name_des']))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8ca7a-90c9-43f6-98f3-d390fae8e77c",
   "metadata": {},
   "source": [
    "# 가중치 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecab7d9-d20e-45f6-b753-7b77d0d5c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "# # 필요한 nltk 데이터 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # 구두점 제거 함수\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 불용어 제거 및 단어의 형태소 분석을 위한 객체 생성\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 입력 설명서 전처리\n",
    "    input_document = remove_punctuation(input_document)\n",
    "    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화\n",
    "    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]\n",
    "    \n",
    "    # HS 코드 문서 전처리\n",
    "    processed_hs_code_documents = {}\n",
    "    for code, doc in hs_code_documents.items():\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokens = word_tokenize(doc.lower())  # 소문자로 변환 및 토큰화\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        processed_hs_code_documents[code] = ' '.join(tokens)\n",
    "    \n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    documents = list(processed_hs_code_documents.values())\n",
    "    tfidf_matrix = tfidf.fit(documents)\n",
    "    documents.append(' '.join(input_tokens))  # 입력 설명서를 단일 문자열로 추가\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix = tfidf.transform(documents)\n",
    "    # tfidf_matrix[:, tfidf.vocabulary_['solely']] *= 2.0\n",
    "    # tfidf_matrix[:, tfidf.vocabulary_['principally']] *= 2.0\n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드 인덱스\n",
    "    most_similar_hs_code_index = similarities.argmax()\n",
    "    \n",
    "    # 유사도가 가장 높은 HS 코드와 유사도 값\n",
    "    most_similar_hs_code = list(hs_code_documents.keys())[most_similar_hs_code_index]\n",
    "    similarity_value = similarities[0][most_similar_hs_code_index]\n",
    "    \n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"유사: {most_similar_hs_code}\")\n",
    "    # print(f\"유사도: {similarity_value}\")\n",
    "    if int(num) == most_similar_hs_code:\n",
    "        cnt += 1 \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bec69f-a5fd-423f-b765-93e4166e437c",
   "metadata": {},
   "source": [
    "# 상위 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c81ed-826e-4009-8c0a-bcd16fe7ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# # 필요한 nltk 데이터 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "cnt = 0\n",
    "# HS 코드와 설명서 데이터\n",
    "hs_code_documents = flattened_dict\n",
    "\n",
    "# 입력 설명서\n",
    "for num, input_document in zip(data['hs code'], data['name_des']):\n",
    "\n",
    "    # 구두점 제거 함수\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 불용어 제거 및 단어의 형태소 분석을 위한 객체 생성\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 입력 설명서 전처리\n",
    "    input_document = remove_punctuation(input_document)\n",
    "    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화\n",
    "    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]\n",
    "    \n",
    "    # HS 코드 문서 전처리\n",
    "    processed_hs_code_documents = {}\n",
    "    for code, doc in hs_code_documents.items():\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokens = word_tokenize(doc.lower())  # 소문자로 변환 및 토큰화\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        processed_hs_code_documents[code] = ' '.join(tokens)\n",
    "    \n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    tfidf = TfidfVectorizer()\n",
    "    documents = list(processed_hs_code_documents.values())\n",
    "    tfidf_matrix = tfidf.fit(documents)\n",
    "    documents.append(' '.join(input_tokens))  # 입력 설명서를 단일 문자열로 추가\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    tfidf_matrix = tfidf.transform(documents)\n",
    "\n",
    "    # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "\n",
    "    \n",
    "    # 유사도가 가장 높은 상위 3개 HS 코드 인덱스 및 값\n",
    "    most_similar_hs_code_indices = np.argsort(similarities)[0][-3:][::-1]  # 상위 3개 인덱스\n",
    "    top_3_similarity_values = similarities[0][most_similar_hs_code_indices] # 상위 3개 유사도 값\n",
    "    top_3_hs_codes = [list(hs_code_documents.keys())[i] for i in most_similar_hs_code_indices]  # 상위 3개 HS 코드\n",
    "    \n",
    "    \n",
    "    # 원래 HS 코드 출력\n",
    "    # print(f'원래 HS 코드: {num}', end=\" \")\n",
    "    # print(f\"상위 3개 유사도 값: {top_3_similarity_values}\")\n",
    "    # print(f\"상위 3개 HS 코드: {top_3_hs_codes}\")\n",
    "    \n",
    "    # 입력 HS 코드와 유사한 상위 3개 HS 코드 중 일치하는 경우 cnt 증가\n",
    "    if int(num) in top_3_hs_codes:\n",
    "        cnt += 1\n",
    "\n",
    "print('맞춘개수: ', cnt)\n",
    "print('예측 정확도: ', (cnt/len(data['name_des']))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
