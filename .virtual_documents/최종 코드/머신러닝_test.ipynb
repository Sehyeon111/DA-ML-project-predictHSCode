


import pandas as pd

df1 = pd.read_excel('../최종 파일/84data_final.xlsx', index_col=0)
df1.head()


df2 = pd.read_excel('../최종 파일/85data_final.xlsx', index_col=0)
df2.head()


product_85 = df2['product']
df_85 = pd.DataFrame({'hs code':df2['hs code'], 'description':df2['description'], 'reason':df2['reason'], 'name_des':df2['name_des']})
df_85.insert(1, 'product', product_85)
df_85.head()


df = pd.concat([df1, df_85])
df.head()


len(df['hs code'].unique())





import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')  # pos_tag를 위한 데이터 다운로드

# Initialize WordNet Lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = stopwords.words('english')
stop_words.append('x')
stop_words.append('w')

lemmatized_des = []
for i in df['name_des']:
    # 텍스트 전처리: 소문자 변환, 토큰화, 불용어 제거
    tokens = word_tokenize(i.lower())
    tagged_tokens = nltk.pos_tag(tokens)  # 단어 토큰에 품사 태깅 추가

    # 품사를 고려하여 단어의 원형 복원 (명사와 동사만 고려)
    filtered_tokens = []
    for word, tag in tagged_tokens:
        if (tag.startswith('NN') or tag.startswith('VB')) and word not in stop_words:  # 명사 또는 동사인 경우
            lemma = lemmatizer.lemmatize(word, pos='n' if tag.startswith('NN') else 'v')
            filtered_tokens.append(lemma)
        elif word.isalpha() and word not in stop_words:
            filtered_tokens.append(word)
    lemmatized_des.append(' '.join(filtered_tokens))

print(lemmatized_des[:4])


preprocessed_df = pd.DataFrame({'hs code': df['hs code'], 'name_des':lemmatized_des})
preprocessed_df





x_train = preprocessed_df['name_des']
y_train = preprocessed_df['hs code']

print(len(x_train))
print(len(y_train))





tfidf_vec = TfidfVectorizer()
tfidf_vec.fit(x_train)
x_train_tfidf_vec = tfidf_vec.transform(x_train) 





from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score,classification_report

svc = LinearSVC(C=1)
svc.fit(x_train_tfidf_vec, y_train)





from sklearn.ensemble import RandomForestClassifier

rmc =RandomForestClassifier(n_estimators=1000, max_depth=500)
rmc.fit(x_train_tfidf_vec,y_train)





from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(solver='lbfgs', multi_class='ovr')
lr.fit(x_train_tfidf_vec,y_train)


# 피클로 모델 저장
import pickle

with open('./drive/MyDrive/DIMA3/lsvc.pkl', 'wb') as file:
    pickle.dump(lsvc, file)


with open('./drive/MyDrive/DIMA3/lsvc.pkl', 'wb') as file:
    pickle.dump(rmc, file)


with open('./drive/MyDrive/DIMA3/lsvc.pkl', 'wb') as file:
    pickle.dump(lr, file)





test = pd.read_excel('../최종 파일/usa_test_data.xlsx', index_col=0)
test.head()


# 테스트 인덱스 값 맞추기
test.reset_index(drop=True, inplace=True)
test


# 학습에 적용된 벡터라이저로 테스트 데이터에도 적용
test_tfidf_vec = tfidf_vec.transform(test)


# 전처리된 테스트 데이터를 예측 모델에 적용
y_hat_lsvc = svc.predict(test_tfidf_vec)
#y_hat_rmc = rmc.predict(test_tfidf_vec)
y_hat_lr = lr.predict(test_tfidf_vec)


# LSVC 정확도 체크
cnt = 0
for i, v in zip(test['hs code'], y_hat_lsvc):
    print(i, v)
    if i == v:
        cnt += 1

print('LSVC 정확도:', cnt/len(test))


# RandomForestClassifier 정확도 체크
cnt = 0
for i, v in zip(test['hs code'], y_hat_rmc):
    if i == v:
        cnt += 1

print('LSVC 정확도:', cnt/len(test))


# LogisticRegression 정확도 체크
cnt = 0
for i, v in zip(test['hs code'], y_hat_lr):
    if i == v:
        cnt += 1

print('LSVC 정확도:', cnt/len(test))
