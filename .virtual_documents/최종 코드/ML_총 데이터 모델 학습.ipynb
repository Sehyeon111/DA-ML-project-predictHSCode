


import pandas as pd

df1 = pd.read_excel('./EU크롤링/84/eu_1213_84_df.xlsx')
df2 = pd.read_excel('./EU크롤링/84/84EU1415_page_.xlsx')
df3 = pd.read_excel('./EU크롤링/84/84_16_17_EU.xlsx')
df4 = pd.read_excel('./EU크롤링/84/EU_84_1819_df.xlsx')
df5 = pd.read_excel('./EU크롤링/84/84_EU_2021_df_total.xlsx')
df6 = pd.read_excel('./EU크롤링/84/84_22_23_EU.xlsx')


df1.info()


df1.drop('Unnamed: 0.1', axis=1, inplace=True)
df1.drop('Unnamed: 0', axis=1, inplace=True)


df1.columns = ['hs code', 'product', 'description', 'reason']


df1.isna().sum()


df1['product'].fillna(' ', inplace=True)


df1.dropna(inplace=True)


df1.info()


df2.info()


df2.drop('Unnamed: 0', axis=1, inplace=True)


df2.isna().sum()


df2['product'].fillna(' ', inplace=True)


df2.info()


df3.info()


df3.drop('Unnamed: 0', axis=1, inplace=True)


df3.isna().sum()


df3['product'].fillna(' ', inplace=True)


df4.info()


df4.drop('Unnamed: 0.1', axis=1, inplace=True)
df4.drop('Unnamed: 0', axis=1, inplace=True)


df4.isna().sum()


df4['product'].fillna(' ', inplace=True)


df4.dropna(inplace=True)


df4 = df4[['hs code', 'product', 'description', 'reason']]


df5.info()


df5.drop('Unnamed: 0', axis=1, inplace=True)


df5.isna().sum()


df6.info()


df6.drop('Unnamed: 0', axis=1, inplace=True)


df6.isna().sum()


df6.dropna(inplace=True)


eu_df = pd.concat([df1, df2, df3, df4, df5, df6])
eu_df.info()


uk_df = pd.read_excel('./EU크롤링/84/84df_UK (1).xlsx')
uk_df.info()


uk_df.drop('Unnamed: 0', axis=1, inplace=True)


ko_df = pd.read_excel('./EU크롤링/84/final84.xlsx')
ko_df.info()


ko_df.drop('Unnamed: 0', axis=1, inplace=True)


total_df = pd.concat([eu_df, uk_df, ko_df])
total_df.info()





total_df


def check_code(code):
    p = re.compile('84\d\d')  # 84로 시작하는 패턴
    m = p.match(str(code))  # hs코드에서 패턴에 맞는 거 찾기 
    
    if m == None:  # 패턴에 맞지 않는 경우(패턴에 맞지 않으면 none 출력)
        return None  # 패턴에 맞지 않으면 na값으로 반환
    
    else:  # 패턴에 맞는 경우
        return m.group()  # 패턴에 맞는 코드(4자리수) 반환


import re
total_df['hs code'] = total_df['hs code'].apply(lambda x : check_code(x))


total_df


total_df.isna().sum()


total_df.dropna(inplace=True)


total_df['name_des']=total_df['product']+' '+total_df['description']


total_df


total_df.to_excel('total_84.xlsx')


import pandas as pd

total_df = pd.read_excel('./EU크롤링/84/total_84.xlsx')
total_df.head()


total_df.info()


total_df.drop('Unnamed: 0', axis=1, inplace=True)


from sklearn.model_selection import train_test_split
import re

x = total_df['name_des'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))
y = total_df['hs code'].astype(int)
z = total_df['reason'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=10)

print(len(x_train))
print(len(x_test))

x_train = pd.concat([x_train, z])
y_train = pd.concat([y_train, y])

print(len(x_train))
print(len(y_train))
print(len(x_test))
print(len(x_test))


from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk import sent_tokenize
from nltk import word_tokenize
import nltk
import string
from nltk.stem import WordNetLemmatizer
import nltk

stopwords = nltk.corpus.stopwords.words('english')
# 특수문자
string = string.punctuation
# 불용어 + 특수문자 합친 리스트 생성
for stri in string:
    stopwords.append(stri)

token_list = None
def text_preprocessing(text):
    # 어근 추출 객체 생성
    lemmar = WordNetLemmatizer()
    global token_list
    # 텍스트를 소문자로 변환 후 문장으로 토큰화
    sentences = sent_tokenize(text.lower())

    for sentence in sentences:        
        # 단어로 토큰화
        tokens = word_tokenize(sentence)
        # 토큰화된 단어들의 어근 추출
        token_list = [lemmar.lemmatize(token) for token in tokens]

    return token_list

tfidf_vec = TfidfVectorizer(tokenizer=text_preprocessing, stop_words = stopwords)
tfidf_vec.fit(x_train)
x_train_tfidf_vec = tfidf_vec.transform(x_train) 

x_test_tfidf_vec = tfidf_vec.transform(x_test)
y_test = y_test.apply(pd.to_numeric, errors='coerce')


from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score

svc = LinearSVC(C=1)
svc.fit(x_train_tfidf_vec,y_train)

y_hat = svc.predict(x_test_tfidf_vec)

print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')
print(f'정밀도: {precision_score(y_test, y_hat, average="weighted"):.3f}')
print(f'재현율: {recall_score(y_test, y_hat, average="weighted"):.3f}')


from sklearn.metrics import classification_report

print(classification_report(y_test, y_hat))


test = pd.read_excel('84미국사례 테스트용.xlsx')
test.head()


x_test_test_tfidf_vec = tfidf_vec.transform(test['description'])


y_hat_hat = svc.predict(x_test_test_tfidf_vec)

y_hat_hat


test['hs code'].values


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score

lr = LogisticRegression(solver='saga', multi_class='multinomial', C=3)
lr.fit(x_train_tfidf_vec,y_train)

y_hat = lr.predict(x_test_tfidf_vec)

print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')
print(f'정밀도: {precision_score(y_test, y_hat, average="weighted"):.3f}')
print(f'재현율: {recall_score(y_test, y_hat, average="weighted"):.3f}')


from sklearn.metrics import classification_report

print(classification_report(y_test, y_hat))


y_hat_hat = lr.predict(x_test_test_tfidf_vec)

y_hat_hat


test['hs code'].values


import numpy as np
import sys

np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(precision=6, suppress=True)


y = total_df['hs code'].unique()
y.sort()


y


lr.predict_proba(x_test_test_tfidf_vec)


p_proba = pd.DataFrame(lr.predict_proba(x_test_test_tfidf_vec), columns=y)


p_proba


p_proba.to_excel('84_predict_proba.xlsx')


test['hs code'].values


# 8481
p_proba.iloc[0].sort_values(ascending=False)[:3]


# 8470
p_proba.iloc[1].sort_values(ascending=False)[:3]


# 8428
p_proba.iloc[2].sort_values(ascending=False)[:3]


# 8445
p_proba.iloc[3].sort_values(ascending=False)[:3]


# 8415
p_proba.iloc[4].sort_values(ascending=False)[:3]


# 8424
p_proba.iloc[5].sort_values(ascending=False)[:3]


# 8482
p_proba.iloc[6].sort_values(ascending=False)[:3]


# 8462
p_proba.iloc[7].sort_values(ascending=False)[:3]


# 8466
p_proba.iloc[8].sort_values(ascending=False)[:3]


# 8414
p_proba.iloc[9].sort_values(ascending=False)[:3]


# 8427
p_proba.iloc[10].sort_values(ascending=False)[:3]


# 8421
p_proba.iloc[11].sort_values(ascending=False)[:3]


# 8473
p_proba.iloc[12].sort_values(ascending=False)[:3]


# 8419
p_proba.iloc[13].sort_values(ascending=False)[:3]


# 8479
p_proba.iloc[14].sort_values(ascending=False)[:3]


# 8422
p_proba.iloc[15].sort_values(ascending=False)[:3]


# 8431
p_proba.iloc[16].sort_values(ascending=False)[:3]


from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score

sv = SVC(kernel='poly', C=3)
sv.fit(x_train_tfidf_vec,y_train)

y_hat = sv.predict(x_test_tfidf_vec)

print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')
print(f'정밀도: {precision_score(y_test, y_hat, average="weighted"):.3f}')
print(f'재현율: {recall_score(y_test, y_hat, average="weighted"):.3f}')


from sklearn.multiclass import OneVsRestClassifier

ovr_clf = OneVsRestClassifier(SVC())
ovr_clf.fit(x_train_tfidf_vec,y_train)
y_hat = sv.predict(x_test_tfidf_vec)

print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')
print(f'정밀도: {precision_score(y_test, y_hat, average="weighted"):.3f}')
print(f'재현율: {recall_score(y_test, y_hat, average="weighted"):.3f}')


y_real_hat = ovr_clf.predict(x_test_test_tfidf_vec)

y_real_hat


test['hs code'].values


test2 = pd.read_excel('EU84_TEST.xlsx')
test2


x_test_test2_tfidf_vec = tfidf_vec.transform(test2['description'])


y_hat_hat2 = ovr_clf.predict(x_test_test2_tfidf_vec)

y_hat_hat2


test2['hs code'].values


from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
nb.fit(x_train_tfidf_vec,y_train)

y_hat = nb.predict(x_test_tfidf_vec)

print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')
print(f'정밀도: {precision_score(y_test, y_hat, average="weighted"):.3f}')
print(f'재현율: {recall_score(y_test, y_hat, average="weighted"):.3f}')


y_hat_hat = nb.predict(x_test_test_tfidf_vec)

y_hat_hat


test['hs code'].values


test2 = pd.read_excel('8479.xlsx')
test2


test2.loc[9]=test2.columns.values


test2


test2.columns = ['hs code', 'description']
test2


x_test_2_tfidf_vec = tfidf_vec.transform(test2['description'])


y_hat_2 = nb.predict(x_test_2_tfidf_vec)

y_hat_2


test2['hs code'].values


y_hat_3 = lr.predict(x_test_2_tfidf_vec)

y_hat_3
