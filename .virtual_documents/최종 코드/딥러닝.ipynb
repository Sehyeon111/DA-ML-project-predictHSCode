


import pandas as pd

df1 = pd.read_excel('./drive/MyDrive/DIMA3/teamproject/최종 파일/84data_final.xlsx', index_col=0)
df1.head()


df2 = pd.read_excel('./drive/MyDrive/DIMA3/teamproject/최종 파일/85data_final.xlsx', index_col=0)
df2.head()


product_85 = df2['product']
df_85 = pd.DataFrame({'hs code':df2['hs code'], 'description':df2['description'], 'reason':df2['reason'], 'name_des':df2['name_des']})
df_85.insert(1, 'product', product_85)
df_85.head()


df = pd.concat([df1, df_85])
df.head()


df.info()





import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')  # pos_tag를 위한 데이터 다운로드

# Initialize WordNet Lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = stopwords.words('english')
stop_words.append('x')
stop_words.append('w')

lemmatized_des = []
for i in df['name_des']:
    # 텍스트 전처리: 소문자 변환, 토큰화, 불용어 제거
    tokens = word_tokenize(i.lower())
    tagged_tokens = nltk.pos_tag(tokens)  # 단어 토큰에 품사 태깅 추가

    # 품사를 고려하여 단어의 원형 복원 (명사와 동사만 고려)
    filtered_tokens = []
    for word, tag in tagged_tokens:
        if (tag.startswith('NN') or tag.startswith('VB')) and word not in stop_words:  # 명사 또는 동사인 경우
            lemma = lemmatizer.lemmatize(word, pos='n' if tag.startswith('NN') else 'v')
            filtered_tokens.append(lemma)
        elif word.isalpha() and word not in stop_words:
            filtered_tokens.append(word)
    lemmatized_des.append(' '.join(filtered_tokens))

print(lemmatized_des[:4])


preprocessed_df = pd.DataFrame({'hs code': df['hs code'], 'name_des':lemmatized_des})
preprocessed_df


# y값 라벨 인코딩
from sklearn.preprocessing import LabelEncoder

encoded = LabelEncoder().fit_transform(preprocessed_df['hs code'])
print(encoded)
preprocessed_df['hs code'] = encoded


preprocessed_df['hs code'][:10]


preprocessed_df['hs code'].unique()


from sklearn.model_selection import train_test_split

x = preprocessed_df['name_des']
y = preprocessed_df['hs code']
z = preprocessed_df['reason']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, shuffle=True, random_state=10)

print(len(x_train))
print(len(y_train))
print(len(x_test))
print(len(x_test))





from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)  # 텍스트 토큰화
X_train_encoded = tokenizer.texts_to_sequences(x_train)  # 토큰화된 텍스트 숫자 라벨링
X_test_encoded = tokenizer.texts_to_sequences(x_test)
print(X_train_encoded[:5])
print(X_test_encoded[:5])


# 학습데이터에 맞게 피팅된 토크나이저 객체 저장
import pickle
with open('lstm_tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)


word_to_index = tokenizer.word_index  # 단어와 숫자 라벨을 딕셔너리로 반환
print(word_to_index)


threshold = 3
total_cnt = len(word_to_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value # 전체 단어 빈도수 구하기

    # 단어의 등장 빈도수가 threshold(3회)보다 작으면, 즉 2회 이하이면
    if (value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합(vocabulary)에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)


import matplotlib.pyplot as plt

print('최대 길이 :',max(len(x) for x in x_train))
print('평균 길이 :',sum(map(len, x_train))/len(x_train))
plt.hist([len(x) for x in x_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()


# 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수
def below_threshold_len(max_len, nested_list):
  count = 0 # 길이 이하인 샘플 수 카운트
  for sentence in nested_list:
    if (len(sentence) <= max_len): # 길이가 max_len 이하이면
        count = count + 1
        
# max_len 이하인 개수 / 전체 샘플 개수
print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100)) 





max_len = 800
below_threshold_len(max_len, x_train)


from keras.utils import pad_sequences

X_train_encoded = pad_sequences(X_train_encoded, maxlen=max_len)
X_test_encoded = pad_sequences(X_test_encoded, maxlen=max_len)


from keras.utils import to_categorical
num_classes = len(preprocessed_df['hs code'].unique())

y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)


from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 128 # 임베딩 벡터 차원 크기
hidden_units = 128 # 은닉상태 크기
vocab_size = len(tokenizer.word_index) + 1
input_data_column = 1

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True))  # 임베딩 층 쌓기
model.add(LSTM(hidden_units)) #,input_shape=(batch_size, input_data_column), return_sequences = True)) # LSTM 층 쌓기
model.add(Dropout(0.25))
model.add(Dense(num_classes, activation='softmax')) # Dense 층 쌓기

# 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 8회 증가하면 정해진 에포크에 도달하지 못하여도 학습을 조기 종료
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)
# 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장
# val_loss가 줄어들다가 증가하는 상황이 오면 과적합으로 판단하기 위함
mc = ModelCheckpoint('./drive/MyDrive/DIMA3/project/DATASET/total data/best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

# 모델을 학습시키기 전에 모델의 손실 함수(loss), 최적화 방법(optimizer), 평가 지표(metrics)를 설정
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


model.summary()


# 모델 학습
history = model.fit(X_train_encoded, y_train, batch_size=64, epochs=50, callbacks=[es,mc],  validation_data=(X_test_encoded, y_test), shuffle=True)


f keras.models import load_model

# 모델 저장
model.save('./drive/MyDrive/DIMA3/model.h5')

# 가중치 저장
model.save_weights('./drive/MyDrive/DIMA3/model_weights.h5')





import pandas as pd
import numpy as np

test = pd.read_excel('/content/drive/MyDrive/DIMA3/project/DATASET/total data/전처리완료된통합검증데이터.xlsx')


print(test)
print(test.info())
test.drop('Unnamed: 0', axis=1, inplace=True)


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')  # pos_tag를 위한 데이터 다운로드

# Initialize WordNet Lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = stopwords.words('english')
stop_words.append('x')
stop_words.append('w')

lemmatized_des = []
for i in test['name_des']:
    # 텍스트 전처리: 소문자 변환, 토큰화, 불용어 제거
    tokens = word_tokenize(i.lower())
    tagged_tokens = nltk.pos_tag(tokens)  # 단어 토큰에 품사 태깅 추가

    # 품사를 고려하여 단어의 원형 복원 (명사와 동사만 고려)
    filtered_tokens = []
    for word, tag in tagged_tokens:
        if (tag.startswith('NN') or tag.startswith('VB')) and word not in stop_words:  # 명사 또는 동사인 경우
            lemma = lemmatizer.lemmatize(word, pos='n' if tag.startswith('NN') else 'v')
            filtered_tokens.append(lemma)
        elif word.isalpha() and word not in stop_words:
            filtered_tokens.append(word)
    lemmatized_des.append(' '.join(filtered_tokens))


pre_test = pd.DataFrame({'hs code': test['hs code'], 'name_des':lemmatized_des})


x_test1 = pre_test['name_des']
y_test1 = pre_test['hs code']


X_test_encoded = tokenizer.texts_to_sequences(x_test1)


from keras.utils import pad_sequences
max_len = 800
X_test_encoded = pad_sequences(X_test_encoded, maxlen=max_len)


from keras.models import load_model

# 저장 모델 불러오기
model = load_model('./drive/MyDrive/DIMA3/project/DATASET/total data/best_model.h5')
# x_test 값 넣어서 예측
y_predict = model.predict(X_test_encoded)


print(y_predict) # 반환값은 확률


# 제일 큰 확률 값 3개 씩 반환해서 hs코드로 바꿔서 보여주기

index_list = []
each_list = []
for k in y_predict:
  each_list = []
  for u in k.argsort()[::-1][:3]:
    for i, v in label_mapping.items():
      if u == i:
        each_list.append(v)
  index_list.append(each_list)

print(index_list)


result = pd.DataFrame({'result': index_list, 'true': y_test1})
display(result)


# 맞춘 갯수 / 전체 갯수
cnt = 0

for i in range(len(result)):
    if result.loc[i]['result'] == result.loc[i]['true']:
        cnt += 1

print('정확도 : ', cnt/len(result))
print('맞춘갯수 : ', cnt)
