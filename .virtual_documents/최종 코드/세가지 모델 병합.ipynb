


import pickle
with open('./최종 파일/lsvc_model.pkl', 'rb') as f:
    lsvc = pickle.load(f)


with open('./최종 파일/lsvc_tfidf_vectorizer.pkl', 'rb') as f:
    tfidf_vec = pickle.load(f)


with open('label_mapping.pkl', 'rb') as f:
    label_mapping = pickle.load(f)


from tensorflow.python.keras.models import load_model

lstm_model = load_model('best_model.h5', compile=False)


from keras.preprocessing.text import Tokenizer

with open('./최종 파일/lstm_tokenizer2.pkl', 'rb') as f:
    tokenizer = pickle.load(f)


with open('cosine_tfidf.pkl', 'rb') as f:
    tfidf = pickle.load(f)


with open('cosine_dict.pkl', 'rb') as f:
    cosine_dict = pickle.load(f)





import string
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))





def three_ensemble_predict(description):
    ## 입력값 전처리
    stop_words = stopwords.words('english')
    stop_words.append('x')
    stop_words.append('w')
    
    # 텍스트 전처리: 소문자 변환, 토큰화, 불용어 제거
    tokens = word_tokenize(description.lower())
    tagged_tokens = nltk.pos_tag(tokens)  # 단어 토큰에 품사 태깅 추가

    lemmatizer = WordNetLemmatizer()
    filtered_tokens = []
    for word, tag in tagged_tokens:
        if (tag.startswith('NN') or tag.startswith('VB')) and word not in stop_words:  # 명사 또는 동사인 경우
            lemma = lemmatizer.lemmatize(word, pos='n' if tag.startswith('NN') else 'v')
            filtered_tokens.append(lemma)
        elif word.isalpha() and word not in stop_words:
            filtered_tokens.append(word)
    preprocessed_description = [' '.join(filtered_tokens)]

    ## LSVC 모델 적용
    description_tfidf_vec = tfidf_vec.transform(preprocessed_description)
    decision_values = lsvc.decision_function(description_tfidf_vec)

    lsvc_top3_list = []
    proba_list = decision_values.argsort().flatten().tolist()[::-1][:3]
    for i in proba_list:
        for k, v in label_mapping.items():
          if k == i:
            lsvc_top3_list.append(v)

    #w1 = 1
    lsvc_top3_dict = {lsvc_top3_list[0]:3, lsvc_top3_list[1]:2, lsvc_top3_list[2]:1}
    #lsvc_top3_dict_w = { k : v*w1 for k, v in lsvc_top3_dict.items() }
    
    ## LSTM 모델 적용
    description_encoded = tokenizer.texts_to_sequences(preprocessed_description)
    description_padded = pad_sequences(description_encoded, maxlen=800)

    des_predict = lstm_model.predict(description_padded)

    lstm_top3_list = []
    proba_list = des_predict.argsort().flatten().tolist()[::-1][:3]
    for i in proba_list:
        for k, v in label_mapping.items():
          if k == i:
            lstm_top3_list.append(v)

    #w2 = 1
    lstm_top3_dict = {lstm_top3_list[0]:3, lstm_top3_list[1]:2, lstm_top3_list[2]:1}
    #lstm_top3_dict_w = { k : v*w2 for k, v in lstm_top3_dict.items() }

    ## 유사도 코드 적용
    # 입력값 전처리
    input_document = remove_punctuation(description)
    input_tokens = word_tokenize(input_document.lower())  # 소문자로 변환 및 토큰화
    input_tokens = [lemmatizer.lemmatize(word) for word in input_tokens if word not in stop_words]

    documents = list(cosine_dict.values())
    documents.append(' '.join(input_tokens))
    
    tfidf_matrix = tfidf.transform(documents)

     # 입력 설명서와 각 HS 코드 간의 코사인 유사도 계산
    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])

    # 유사도가 가장 높은 상위 3개 HS 코드 인덱스 및 값
    most_similar_hs_code_indices = np.argsort(similarities)[0][-3:][::-1]  # 상위 3개 인덱스
    similarities_top3_list = [list(cosine_dict.keys())[i] for i in most_similar_hs_code_indices]  # 상위 3개 HS 코드

    #w3 = 1
    similarities_top3_dict = {similarities_top3_list[0]:3, similarities_top3_list[1]:2, similarities_top3_list[2]:1}
    #similarities_top3_dict_w = { k : v*w3 for k, v in similarities_top3_dict.items() }
    
    ## 딕셔너리 병합 및 값 더하기
    merged_dict = {}
    for d in [lsvc_top3_dict, lstm_top3_dict, similarities_top3_dict]:
        for k, v in d.items():
            merged_dict[k] = merged_dict.get(k, 0) + v
    
    # 상위 3개 HS 코드 가져오기
    top_3_hs_codes = dict(sorted(merged_dict.items(), key=lambda x: x[1], reverse=True)[:3])
    
    return list(top_3_hs_codes.keys())


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from keras.utils import pad_sequences
import numpy as np

three_ensemble_predict('Optical wireless mouse - three push button switches, scroll wheel, laser sensor, transmitter and Bluetooth transmitter in a plastic casing with a USB connection (dimensions: 106 x 57 x 35 mm), (Figure attached) - for entry of position data and to trigger Control commands, - to the central unit of automatic data processing machines directly connected and able to provide data in a system-usable form - together with accessory pack (user manual, 2 AAA rechargeable batteries and USB charge cable) in a joint Verkaufsumschließung. The optical mouse determines the character of the whole. "input device for automatic data processing machines, no keyboard, not intended for use in civil aircraft - optical computer mouse in the set with no charakterbestimmendem pack.')





import pandas as pd

test = pd.read_excel('./최종 파일/전처리완료된통합검증데이터.xlsx', index_col = 0)
test.head()


test.reset_index(drop=True, inplace=True)


test['hs code'].astype(object)


cnt = 0
cnt_top = 0
for i, v in zip(test['hs code'], test['name_des']):
    result = three_ensemble_predict(v)
    if i in result:
        cnt += 1 
    if i == result[0]:
        cnt_top += 1
        
print(cnt)
print(cnt/len(test))

print(cnt_top)
print(cnt_top/len(test))


usa_test1 = pd.read_excel('84미국사례 테스트용.xlsx', index_col=0)
usa_test2 = pd.read_excel('85미국사례 테스트용.xlsx', index_col=0)

usa_test1.head()


usa_test2.head()


usa_test = pd.concat([usa_test1, usa_test2])
usa_test.info()


usa_test.reset_index(inplace=True)
usa_test.info()


usa_test.head()


usa_test.to_excel('미국사례테스트용.xlsx')


cnt = 0
cnt_top = 0
for i, v in zip(usa_test['hs code'], usa_test['description']):
    pred = three_ensemble_predict(v)
    if i in pred:
        cnt += 1 
    if i == pred[0]:
        cnt_top += 1

print('<세가지 모델 합친 것 미국사례 정확도>')
print('3개중 하나 맞춘 개수:', cnt)
print('3개중 하나 맞춘 정확도:', cnt/len(usa_test))

print('top1 맞춘 개수:', cnt_top)
print('top1 맞춘 개수:', cnt_top/len(usa_test))





usa_total = pd.read_excel('미국테스트전체데이터.xlsx', index_col=0)
usa_total.head()


usa_total.reset_index(drop=True, inplace=True)
usa_total


cnt = 0
cnt_top = 0
for i, v in zip(usa_total['hs code'], usa_total['description']):
    pred = three_ensemble_predict(v)
    if i in pred:
        cnt += 1 
    if i == pred[0]:
        cnt_top += 1

print('<세가지 모델 합친 것 미국사례 정확도>')
print('3개중 하나 맞춘 개수:', cnt)
print('3개중 하나 맞춘 정확도:', round(cnt/len(usa_total),3))

print('top1 맞춘 개수:', cnt_top)
print('top1 맞춘 개수:', round(cnt_top/len(usa_total),3))
